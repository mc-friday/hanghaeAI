{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mc-friday/hanghaeAI/blob/main/1_1_Linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression 실습\n",
        "\n",
        "이번 실습에서는 linear regression에 대한 gradient descent를 직접 구현해봅니다. 여기서 사용할 문제들은 크게 두 가지로 OR 문제와 XOR 문제입니다.\n",
        "\n",
        "먼저 필요한 library들을 import합시다."
      ],
      "metadata": {
        "id": "dS_t0-ik_WC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DEJFJkL6qHB9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OR Problem\n",
        "\n",
        "OR은 0 또는 1의 값을 가질 수 있는 두 개의 정수를 입력으로 받아 둘 중에 하나라도 1이면 1을 출력하고 아니면 0을 출력하는 문제입니다.\n",
        "즉, 우리가 학습하고자 하는 함수는 2개의 정수를 입력받아 하나의 정수를 출력하면됩니다. 이러한 함수를 학습하기 위한 data는 다음과 같이 구성할 수 있습니다."
      ],
      "metadata": {
        "id": "cG2fJsOF8LsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([\n",
        "    [0., 0.],\n",
        "    [0., 1.],\n",
        "    [1., 0.],\n",
        "    [1., 1.]\n",
        "])\n",
        "y = torch.tensor([0, 1, 1, 1])\n",
        "\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "SsEdD6T7qLJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c91b61-98b4-45af-be7b-865abe2c8b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2]) torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 결과에서 볼 수 있다시피 $x$의 shape은 (4, 2)로, 총 4개의 two-dimensional data 임을 알 수 있습니다. $y$는 각 $x_i$에 대한 label로 우리가 설정한 문제의 조건을 잘 따라가는 것을 알 수 있습니다.\n",
        "\n",
        "다음으로는 linear regression의 parameter들인 $w, b$를 정의하겠습니다."
      ],
      "metadata": {
        "id": "YyD1n6wf_3ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn((1, 2))\n",
        "b = torch.randn((1, 1))\n",
        "\n",
        "print(w.shape, b.shape)"
      ],
      "metadata": {
        "id": "uzG4w1VYqlhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c5ad5c5-dea3-4b59-cebe-feeca50507f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2]) torch.Size([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$w$는 1x2의 벡터이고 $b$는 1x1의 scalar임을 알 수 있습니다. 여기서는 `torch.randn`을 사용하여 standard normal distribution을 가지고 초기화하였습니다.\n",
        "\n",
        "이러한 $w, b$와 data $x, y$가 주어졌을 때 우리가 학습한 $w, b$의 성능을 평가하는 함수를 구현합시다.\n",
        "평가 함수는 다음과 같이 MSE로 정의됩니다:\n",
        "$$l(f) := MSE(f(x), y) = \\frac{1}{n} \\sum_{i=1}^n (f(x_i) - y)^2.$$\n",
        "이를 구현한 코드는 다음과 같습니다."
      ],
      "metadata": {
        "id": "ELTb9Dl-AYbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(w, b, x):\n",
        "  return torch.matmul(w, x.T) + b\n",
        "\n",
        "\n",
        "def loss(w, b, x, y):\n",
        "  return (y - pred(w, b, x)).pow(2).mean()"
      ],
      "metadata": {
        "id": "LBxldV7D8UMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 `def pred(w, b, x)`는 $wx^T + b$, 즉 1차 함수 $f$의 $x$에 대한 결과를 반환하는 함수를 구현했습니다.\n",
        "이를 이용하여 주어진 label $y$와의 MSE를 측정하는 코드가 `def loss(w, b, x, y)`에 구현되어있습니다.\n",
        "\n",
        "다음은 MSE를 기반으로 $w, b$의 gradient를 구하는 코드를 구현하겠습니다.\n",
        "MSE에 대한 $w$의 gradient는 다음과 같이 구할 수 있습니다:\n",
        "$$\\frac{\\partial l}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y)x_i.$$\n",
        "$b$에 대한 gradient는 다음과 같습니다:\n",
        "$$\\frac{\\partial l}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y).$$\n",
        "이를 코드로 구현하면 다음과 같습니다."
      ],
      "metadata": {
        "id": "gmM79Ly6VyBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_w(w, b, x, y):\n",
        "  # w: (1, d), b: (1, 1), x: (n, d), y: (n)\n",
        "  tmp1 = torch.matmul(w, x.T)  # (1, n)\n",
        "  tmp2 = tmp1 + b              # (1, n)\n",
        "  tmp3 = 2 * (tmp2 - y[None])  # (1, n)\n",
        "  grad_item = tmp3.T * x       # (n, d)\n",
        "  return grad_item.mean(dim=0, keepdim=True)  # (1, d)\n",
        "\n",
        "\n",
        "def grad_b(w, b, x, y):\n",
        "  # w: (1, d), b: (1, 1), x: (n, d), y: (n)\n",
        "  grad_item = 2 * (torch.matmul(w, x.T) + b - y[None])  # (1, n)\n",
        "  return grad_item.mean(dim=-1, keepdim=True)           # (1, 1)"
      ],
      "metadata": {
        "id": "rLrsXZ0iq13m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 중요한 것은 shape에 맞춰서 연산을 잘 사용해야한다는 것입니다. Shape과 관련된 설명은 `[Chapter 0]`의 Numpy에서 설명했으니, 복습하신다는 느낌으로 주석으로 써놓은 shape들을 유도해보시면 좋을 것 같습니다. 중요한 것은 반환되는 tensor의 shape이 우리가 구하고자 하는 gradient와 일치해야 한다는 것입니다. 예를 들어 $w$의 $l$에 대한 gradient는 $w$와 shape이 동일해야 합니다.\n",
        "\n",
        "마지막으로 gradient descent 함수를 구현하겠습니다. Gradient descent는 다음과 같이 정의됩니다:\n",
        "$$w^{(new)} = w^{(old)} - \\eta \\frac{\\partial l}{\\partial w} \\biggr\\rvert_{w = w^{(old)}}.$$\n",
        "Gradient는 위에서 구현했으니 이를 활용하여 learning rate $\\eta$가 주어졌을 때 $w, b$를 update하는 코드를 구현할 수 있습니다. 구현한 결과는 다음과 같습니다."
      ],
      "metadata": {
        "id": "mCbBU1RaX6O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update(x, y, w, b, lr):\n",
        "  w = w - lr * grad_w(w, b, x, y)\n",
        "  b = b - lr * grad_b(w, b, x, y)\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "wFRS72UF8QVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent에 해당하는 코드는 모두 구현하였습니다. 이제 학습하는 코드를 구현하겠습니다:"
      ],
      "metadata": {
        "id": "b93uvneVZ7bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_epochs, lr, w, b, x, y):\n",
        "  for e in range(n_epochs):\n",
        "    w, b = update(x, y, w, b, lr)\n",
        "    print(f\"Epoch {e:3d} | Loss: {loss(w, b, x, y)}\")\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "Pa6fA_ZUFI-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기서 `n_epochs`는 update를 하는 횟수를 의미합니다. 매 update 이후에 `loss` 함수를 사용하여 잘 수렴하고 있는지 살펴봅니다. 실제로 이 함수를 실행한 결과는 다음과 같습니다."
      ],
      "metadata": {
        "id": "GrJGKWilaBFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "lr = 0.1\n",
        "\n",
        "w, b = train(n_epochs, lr, w, b, x, y)\n",
        "print(w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFk-josgBSj7",
        "outputId": "ec127ca9-a563-43ac-8adc-8e1e398ecb5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Loss: 7.850151062011719\n",
            "Epoch   1 | Loss: 4.1046576499938965\n",
            "Epoch   2 | Loss: 2.243908166885376\n",
            "Epoch   3 | Loss: 1.3163319826126099\n",
            "Epoch   4 | Loss: 0.850930392742157\n",
            "Epoch   5 | Loss: 0.6146091222763062\n",
            "Epoch   6 | Loss: 0.4920266270637512\n",
            "Epoch   7 | Loss: 0.4261135458946228\n",
            "Epoch   8 | Loss: 0.38862910866737366\n",
            "Epoch   9 | Loss: 0.36559048295021057\n",
            "Epoch  10 | Loss: 0.3500640094280243\n",
            "Epoch  11 | Loss: 0.33859962224960327\n",
            "Epoch  12 | Loss: 0.32946884632110596\n",
            "Epoch  13 | Loss: 0.32179340720176697\n",
            "Epoch  14 | Loss: 0.3151160180568695\n",
            "Epoch  15 | Loss: 0.30918800830841064\n",
            "Epoch  16 | Loss: 0.3038652241230011\n",
            "Epoch  17 | Loss: 0.2990560531616211\n",
            "Epoch  18 | Loss: 0.29469650983810425\n",
            "Epoch  19 | Loss: 0.2907375991344452\n",
            "Epoch  20 | Loss: 0.2871391475200653\n",
            "Epoch  21 | Loss: 0.2838665246963501\n",
            "Epoch  22 | Loss: 0.2808893918991089\n",
            "Epoch  23 | Loss: 0.27818039059638977\n",
            "Epoch  24 | Loss: 0.27571505308151245\n",
            "Epoch  25 | Loss: 0.2734709084033966\n",
            "Epoch  26 | Loss: 0.2714279294013977\n",
            "Epoch  27 | Loss: 0.2695675492286682\n",
            "Epoch  28 | Loss: 0.26787328720092773\n",
            "Epoch  29 | Loss: 0.2663300037384033\n",
            "Epoch  30 | Loss: 0.2649238109588623\n",
            "Epoch  31 | Loss: 0.2636423707008362\n",
            "Epoch  32 | Loss: 0.26247429847717285\n",
            "Epoch  33 | Loss: 0.26140928268432617\n",
            "Epoch  34 | Loss: 0.26043811440467834\n",
            "Epoch  35 | Loss: 0.25955232977867126\n",
            "Epoch  36 | Loss: 0.2587440609931946\n",
            "Epoch  37 | Loss: 0.25800636410713196\n",
            "Epoch  38 | Loss: 0.25733304023742676\n",
            "Epoch  39 | Loss: 0.256718248128891\n",
            "Epoch  40 | Loss: 0.2561566233634949\n",
            "Epoch  41 | Loss: 0.25564366579055786\n",
            "Epoch  42 | Loss: 0.25517499446868896\n",
            "Epoch  43 | Loss: 0.2547464072704315\n",
            "Epoch  44 | Loss: 0.2543547749519348\n",
            "Epoch  45 | Loss: 0.25399649143218994\n",
            "Epoch  46 | Loss: 0.25366872549057007\n",
            "Epoch  47 | Loss: 0.2533688545227051\n",
            "Epoch  48 | Loss: 0.2530944049358368\n",
            "Epoch  49 | Loss: 0.25284305214881897\n",
            "Epoch  50 | Loss: 0.2526128888130188\n",
            "Epoch  51 | Loss: 0.25240206718444824\n",
            "Epoch  52 | Loss: 0.2522089183330536\n",
            "Epoch  53 | Loss: 0.2520318627357483\n",
            "Epoch  54 | Loss: 0.25186944007873535\n",
            "Epoch  55 | Loss: 0.2517205476760864\n",
            "Epoch  56 | Loss: 0.2515839636325836\n",
            "Epoch  57 | Loss: 0.25145867466926575\n",
            "Epoch  58 | Loss: 0.25134357810020447\n",
            "Epoch  59 | Loss: 0.2512379586696625\n",
            "Epoch  60 | Loss: 0.251140832901001\n",
            "Epoch  61 | Loss: 0.25105172395706177\n",
            "Epoch  62 | Loss: 0.25096988677978516\n",
            "Epoch  63 | Loss: 0.25089457631111145\n",
            "Epoch  64 | Loss: 0.25082531571388245\n",
            "Epoch  65 | Loss: 0.25076162815093994\n",
            "Epoch  66 | Loss: 0.2507030665874481\n",
            "Epoch  67 | Loss: 0.25064921379089355\n",
            "Epoch  68 | Loss: 0.25059953331947327\n",
            "Epoch  69 | Loss: 0.25055381655693054\n",
            "Epoch  70 | Loss: 0.2505117654800415\n",
            "Epoch  71 | Loss: 0.2504730224609375\n",
            "Epoch  72 | Loss: 0.2504372000694275\n",
            "Epoch  73 | Loss: 0.2504042983055115\n",
            "Epoch  74 | Loss: 0.250373899936676\n",
            "Epoch  75 | Loss: 0.25034597516059875\n",
            "Epoch  76 | Loss: 0.250320166349411\n",
            "Epoch  77 | Loss: 0.2502962350845337\n",
            "Epoch  78 | Loss: 0.2502741813659668\n",
            "Epoch  79 | Loss: 0.2502538561820984\n",
            "Epoch  80 | Loss: 0.25023511052131653\n",
            "Epoch  81 | Loss: 0.25021785497665405\n",
            "Epoch  82 | Loss: 0.25020185112953186\n",
            "Epoch  83 | Loss: 0.2501869797706604\n",
            "Epoch  84 | Loss: 0.25017330050468445\n",
            "Epoch  85 | Loss: 0.25016075372695923\n",
            "Epoch  86 | Loss: 0.25014904141426086\n",
            "Epoch  87 | Loss: 0.25013816356658936\n",
            "Epoch  88 | Loss: 0.2501281499862671\n",
            "Epoch  89 | Loss: 0.2501189112663269\n",
            "Epoch  90 | Loss: 0.2501102685928345\n",
            "Epoch  91 | Loss: 0.25010237097740173\n",
            "Epoch  92 | Loss: 0.25009503960609436\n",
            "Epoch  93 | Loss: 0.2500881850719452\n",
            "Epoch  94 | Loss: 0.2500818967819214\n",
            "Epoch  95 | Loss: 0.2500761151313782\n",
            "Epoch  96 | Loss: 0.2500706911087036\n",
            "Epoch  97 | Loss: 0.2500656247138977\n",
            "Epoch  98 | Loss: 0.2500609755516052\n",
            "Epoch  99 | Loss: 0.250056654214859\n",
            "tensor([[0.5068, 0.5028]]) tensor([[0.2444]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "잘 수렴하는 것을 확인하였습니다. 마지막으로 OR data에 대한 $w, b$의 예측 결과와 label을 비교해봅시다."
      ],
      "metadata": {
        "id": "y2Ny-YkAaNh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred(w, b, x))\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggGP969Bh-w",
        "outputId": "c9a5ccb0-5fdc-4f86-ed30-157f56f97d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2444, 0.7472, 0.7512, 1.2540]])\n",
            "tensor([0, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측 결과를 볼 수 있다시피 우리의 linear regression model은 0과 1에 해당하는 data를 잘 구분하는 것을 알 수 있습니다."
      ],
      "metadata": {
        "id": "F8gKvx2naWDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XOR Problem\n",
        "\n",
        "이번에는 XOR를 학습해보겠습니다. XOR은 OR과 똑같은 입력을 받는 문제로, 두 개의 0 또는 1의 정수가 들어왔을 때 두 정수가 다르면 1, 아니면 0을 출력해야 합니다.\n",
        "먼저 data를 만들어보겠습니다:"
      ],
      "metadata": {
        "id": "zMXZLfd3DC50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([\n",
        "    [0., 0.],\n",
        "    [0., 1.],\n",
        "    [1., 0.],\n",
        "    [1., 1.]\n",
        "])\n",
        "y = torch.tensor([0, 1, 1, 0])\n",
        "\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtFGsqNXCjtM",
        "outputId": "7183941e-8298-4f2d-f443-9c20f39aec11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2]) torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "보시다시피 shape이나 생성 과정은 OR과 똑같습니다. 다른 것은 $y$에서의 labeling입니다. OR과 다르게 $x = (1, 1)$에 대해서는 0을 labeling했습니다.\n",
        "이러한 사소한 차이에 대해서도 linear regression model이 잘 학습할 수 있을지 살펴보겠습니다."
      ],
      "metadata": {
        "id": "iYRtKaviaedO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "lr = 0.1\n",
        "\n",
        "w, b = train(n_epochs, lr, w, b, x, y)\n",
        "print(w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw5UUqKdDG98",
        "outputId": "b37eb7e7-ff7f-4887-e432-c5f597179f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Loss: 1.5148558616638184\n",
            "Epoch   1 | Loss: 1.3833328485488892\n",
            "Epoch   2 | Loss: 1.3084571361541748\n",
            "Epoch   3 | Loss: 1.262673258781433\n",
            "Epoch   4 | Loss: 1.2321398258209229\n",
            "Epoch   5 | Loss: 1.2098617553710938\n",
            "Epoch   6 | Loss: 1.1922695636749268\n",
            "Epoch   7 | Loss: 1.1775166988372803\n",
            "Epoch   8 | Loss: 1.1646277904510498\n",
            "Epoch   9 | Loss: 1.1530711650848389\n",
            "Epoch  10 | Loss: 1.1425437927246094\n",
            "Epoch  11 | Loss: 1.1328625679016113\n",
            "Epoch  12 | Loss: 1.1239094734191895\n",
            "Epoch  13 | Loss: 1.1156009435653687\n",
            "Epoch  14 | Loss: 1.1078746318817139\n",
            "Epoch  15 | Loss: 1.1006799936294556\n",
            "Epoch  16 | Loss: 1.0939748287200928\n",
            "Epoch  17 | Loss: 1.0877225399017334\n",
            "Epoch  18 | Loss: 1.0818901062011719\n",
            "Epoch  19 | Loss: 1.0764483213424683\n",
            "Epoch  20 | Loss: 1.0713697671890259\n",
            "Epoch  21 | Loss: 1.0666295289993286\n",
            "Epoch  22 | Loss: 1.062205195426941\n",
            "Epoch  23 | Loss: 1.058075189590454\n",
            "Epoch  24 | Loss: 1.0542194843292236\n",
            "Epoch  25 | Loss: 1.050620198249817\n",
            "Epoch  26 | Loss: 1.047260046005249\n",
            "Epoch  27 | Loss: 1.0441229343414307\n",
            "Epoch  28 | Loss: 1.0411943197250366\n",
            "Epoch  29 | Loss: 1.0384598970413208\n",
            "Epoch  30 | Loss: 1.0359071493148804\n",
            "Epoch  31 | Loss: 1.033523678779602\n",
            "Epoch  32 | Loss: 1.0312986373901367\n",
            "Epoch  33 | Loss: 1.0292211771011353\n",
            "Epoch  34 | Loss: 1.0272817611694336\n",
            "Epoch  35 | Loss: 1.0254709720611572\n",
            "Epoch  36 | Loss: 1.0237804651260376\n",
            "Epoch  37 | Loss: 1.0222020149230957\n",
            "Epoch  38 | Loss: 1.0207284688949585\n",
            "Epoch  39 | Loss: 1.019352674484253\n",
            "Epoch  40 | Loss: 1.0180680751800537\n",
            "Epoch  41 | Loss: 1.0168689489364624\n",
            "Epoch  42 | Loss: 1.015749216079712\n",
            "Epoch  43 | Loss: 1.0147039890289307\n",
            "Epoch  44 | Loss: 1.0137280225753784\n",
            "Epoch  45 | Loss: 1.0128167867660522\n",
            "Epoch  46 | Loss: 1.0119661092758179\n",
            "Epoch  47 | Loss: 1.011171817779541\n",
            "Epoch  48 | Loss: 1.0104304552078247\n",
            "Epoch  49 | Loss: 1.0097380876541138\n",
            "Epoch  50 | Loss: 1.009091854095459\n",
            "Epoch  51 | Loss: 1.008488416671753\n",
            "Epoch  52 | Loss: 1.0079249143600464\n",
            "Epoch  53 | Loss: 1.0073989629745483\n",
            "Epoch  54 | Loss: 1.0069078207015991\n",
            "Epoch  55 | Loss: 1.0064493417739868\n",
            "Epoch  56 | Loss: 1.00602126121521\n",
            "Epoch  57 | Loss: 1.0056216716766357\n",
            "Epoch  58 | Loss: 1.0052485466003418\n",
            "Epoch  59 | Loss: 1.0049002170562744\n",
            "Epoch  60 | Loss: 1.0045748949050903\n",
            "Epoch  61 | Loss: 1.0042712688446045\n",
            "Epoch  62 | Loss: 1.0039877891540527\n",
            "Epoch  63 | Loss: 1.00372314453125\n",
            "Epoch  64 | Loss: 1.0034760236740112\n",
            "Epoch  65 | Loss: 1.0032453536987305\n",
            "Epoch  66 | Loss: 1.0030298233032227\n",
            "Epoch  67 | Loss: 1.00282883644104\n",
            "Epoch  68 | Loss: 1.002640962600708\n",
            "Epoch  69 | Loss: 1.0024657249450684\n",
            "Epoch  70 | Loss: 1.0023020505905151\n",
            "Epoch  71 | Loss: 1.002149224281311\n",
            "Epoch  72 | Loss: 1.0020066499710083\n",
            "Epoch  73 | Loss: 1.0018733739852905\n",
            "Epoch  74 | Loss: 1.001749038696289\n",
            "Epoch  75 | Loss: 1.0016329288482666\n",
            "Epoch  76 | Loss: 1.0015246868133545\n",
            "Epoch  77 | Loss: 1.0014234781265259\n",
            "Epoch  78 | Loss: 1.001328945159912\n",
            "Epoch  79 | Loss: 1.0012407302856445\n",
            "Epoch  80 | Loss: 1.001158356666565\n",
            "Epoch  81 | Loss: 1.0010815858840942\n",
            "Epoch  82 | Loss: 1.0010097026824951\n",
            "Epoch  83 | Loss: 1.0009427070617676\n",
            "Epoch  84 | Loss: 1.0008801221847534\n",
            "Epoch  85 | Loss: 1.0008217096328735\n",
            "Epoch  86 | Loss: 1.0007672309875488\n",
            "Epoch  87 | Loss: 1.000716209411621\n",
            "Epoch  88 | Loss: 1.0006687641143799\n",
            "Epoch  89 | Loss: 1.0006242990493774\n",
            "Epoch  90 | Loss: 1.0005829334259033\n",
            "Epoch  91 | Loss: 1.0005443096160889\n",
            "Epoch  92 | Loss: 1.0005080699920654\n",
            "Epoch  93 | Loss: 1.0004743337631226\n",
            "Epoch  94 | Loss: 1.0004428625106812\n",
            "Epoch  95 | Loss: 1.000413417816162\n",
            "Epoch  96 | Loss: 1.0003859996795654\n",
            "Epoch  97 | Loss: 1.000360369682312\n",
            "Epoch  98 | Loss: 1.0003364086151123\n",
            "Epoch  99 | Loss: 1.0003141164779663\n",
            "tensor([[0.0122, 0.0122]]) tensor([[0.4858]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전과는 다르게 loss가 1.0보다 작아지지 않는 것을 알 수 있습니다. 실제 예측 결과를 살펴보면 다음과 같습니다."
      ],
      "metadata": {
        "id": "i8sMLaJ9a770"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred(w, b, x))\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L81iXxgHDIq2",
        "outputId": "1fc0cef5-f24d-45e7-bc0a-fc91c82762b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4858, 0.4980, 0.4980, 0.5102]])\n",
            "tensor([0, 1, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "보시다시피 0과 1에 해당하는 data들을 잘 구분하지 못하는 모습니다. Linear regression model은 XOR을 잘 처리하지 못하는 것을 우리는 이번 실습을 통해 알 수 있습니다."
      ],
      "metadata": {
        "id": "MuqkwJ2NbB7S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zAy7YgFDMgx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}