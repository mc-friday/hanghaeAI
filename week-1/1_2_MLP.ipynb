{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mc-friday/hanghaeAI/blob/main/1_2_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP 실습\n",
        "\n",
        "이번 실습에서는 PyTorch에서 MLP 및 backpropagation을 구현하여 XOR 문제에 학습해봅니다. 먼저 필요한 library들을 import합시다."
      ],
      "metadata": {
        "id": "Tkat-RaWc1u4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEJFJkL6qHB9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그 다음 공유된 notebook과 똑같은 결과를 얻기 위한 코드를 구현합니다."
      ],
      "metadata": {
        "id": "UC0DqhmtdBuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "seed = 7777\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "nnXpVhJXbRzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 코드에서는 seed 고정과 `cudnn.deterministic`, `cudnn.benchmark`를 조정하였습니다. 자세한 사항들은 https://pytorch.org/docs/stable/notes/randomness.html에서 찾아볼 수 있습니다.\n",
        "\n",
        "다음은 이전 실습과 같은 코드로 XOR data를 생성하는 과정입니다."
      ],
      "metadata": {
        "id": "vmAh1sEJdKUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([\n",
        "    [0., 0.],\n",
        "    [0., 1.],\n",
        "    [1., 0.],\n",
        "    [1., 1.]\n",
        "])\n",
        "y = torch.tensor([0, 1, 1, 0])\n",
        "\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "SsEdD6T7qLJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a82aae-b239-4f92-b1f8-2745101a69bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2]) torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data를 만들었으니 이제 MLP를 구현해야 합니다. 여기서 알아둬야할 점들은 다음과 같습니다:\n",
        "- PyTorch에서는 우리가 학습하고자 하는 함수 $f$를 보통 `torch.nn.Module` class를 상속받은 class로 구현합니다.\n",
        "- `torch.nn.Module` class는 abstract class로, `def forward`를 구현하도록 abstractmethod를 제공합니다. 이 method는 $f(x)$, 즉 함수의 출력에 해당됩니다.\n",
        "- PyTorch에서는 선형함수를 `torch.nn.Linear` class로 구현할 수 있습니다.\n",
        "- 마찬가지로 ReLU도 `torch.nn.ReLU`로 제공하고 있습니다.\n",
        "\n",
        "위의 점들을 활용하여 2-layer MLP를 구현한 결과는 다음과 같습니다."
      ],
      "metadata": {
        "id": "5sSAu0HKdlRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, d, d_prime):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(d, d_prime)\n",
        "    self.layer2 = nn.Linear(d_prime, 1)\n",
        "    self.act = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (n, d)\n",
        "    x = self.layer1(x)  # (n, d_prime)\n",
        "    x = self.act(x)     # (n, d_prime)\n",
        "    x = self.layer2(x)  # (n, 1)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = Model(2, 10)"
      ],
      "metadata": {
        "id": "KZe7xFjMqfc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리가 원하는 것은 $x \\in \\mathbb{R}^{n \\times d}$가 입력으로 들어왔을 때 첫 번째 선형 함수를 거쳤을 때의 dimension이 $\\mathbb{R}^{n \\times d'}$이 되면서 두 번째 선형 함수를 거쳤을 때는 $\\mathbb{R}^{n \\times 1}$이 되기를 원합니다.\n",
        "첫 번째 선형함수는 `self.layer1 = nn.Linear(d, d_prime)`로 구현할 수 있으며, `nn.Linear`의 첫 번째 인자는 입력으로 들어오는 tensor의 마지막 dimension, 두 번째 인자는 출력되는 tensor의 마지막 dimension을 뜻합니다.\n",
        "이 정보를 이용하면 두 번째 선형함수도 `self.layer2 = nn.Linear(d_prime, 1)`로 구현할 수 있습니다.\n",
        "마찬가지로 ReLU도 `nn.ReLU()`로 선언할 수 있습니다.\n",
        "\n",
        "이제 $f(x)$에 대한 구현은 `def forward(self, x)`에서 할 수 있습니다.\n",
        "생성자에서 선언한 세 개의 layer들을 순차적으로 지나면 우리가 원하던 결과를 얻을 수 있습니다.\n",
        "여기서도 shape의 변화를 돌려보지 않고 예측하는 것이 실제로 구현하고 디버깅할 때 중요하게 여겨집니다.\n",
        "\n",
        "마지막 줄에서는 입력 dimension이 2이고, 중간 dimension이 10이 되는 2-layer MLP에 대한 object를 생성하였습니다.\n",
        "다음은 PyTorch에서 gradient descent를 어떻게 구현하는지 살펴봅시다."
      ],
      "metadata": {
        "id": "XjPzBGayeZ2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "NpygQ87grSJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch는 다양한 update 알고리즘들을 `torch.optim`에서 제공하고 있습니다.\n",
        "우리는 gradient descent를 사용할 것이기 때문에 `SGD` class를 불러옵니다.\n",
        "`SGD`는 첫 번째 인자로 업데이트를 할 parameter들의 list를 받습니다. 예를 들어 선형 함수에서의 $w, b$가 있습니다.\n",
        "PyTorch의 `nn.Module` class는 이러한 것들을 잘 정의해주기 때문에 `model.parameters()`의 형식으로 넘겨주기만 하면 됩니다.\n",
        "두 번째 인자는 learning rate로, 이전의 gradient descent에서 사용하던 learning rate와 똑같은 역할을 가지고 있습니다.\n",
        "\n",
        "다음은 실제 학습 코드를 구현하며, backpropagation이 어떻게 이루어지는지 살펴봅시다."
      ],
      "metadata": {
        "id": "HraI34csgTYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_epochs, model, optimizer, x, y):\n",
        "  for e in range(n_epochs):\n",
        "    model.zero_grad()\n",
        "\n",
        "    y_pred = model(x)\n",
        "    loss = (y_pred[:, 0] - y).pow(2).sum()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {e:3d} | Loss: {loss}\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "Pa6fA_ZUFI-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model(x)`를 통해 우리가 구현한 `forward` 함수를 사용할 수 있습니다. 그리고 이를 이용하여 MSE loss로 `model`을 평가할 수 있습니다.\n",
        "Gradient descent의 구현은 다음 세 가지 부분에서 진행되고 있습니다:\n",
        "- 본격적으로 학습이 진행되기 전에 `model.zero_grad()`를 실행합니다. 이는 각 parameter의 gradient 값이 저장되어 있을 수도 있기 때문에 지워주는 기능을 수행합니다.\n",
        "- loss를 계산한 후, `loss.backward()`를 진행합니다. backward 함수를 실행하면 `model`에 있는 모든 parameter들의 `loss`에 대한 gradient를 계산하게 됩니다.\n",
        "- 마지막으로 계산한 gradient들을 가지고 parameter들을 update하는 것은 `optimizer.step()`을 통해 진행하게 됩니다. optimizer는 이전에 인자로 받았던 `model.parameters()`에 해당하는 parameter들만 update를 진행하게 됩니다.\n",
        "\n",
        "위의 코드로 학습을 진행한 코드는 다음과 같습니다."
      ],
      "metadata": {
        "id": "heaHQYZVhBAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 100\n",
        "model = train(n_epochs, model, optimizer, x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFk-josgBSj7",
        "outputId": "10343a67-4055-4b7d-e103-34a1d043d01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Loss: 2.8124639987945557\n",
            "Epoch   1 | Loss: 2.309718132019043\n",
            "Epoch   2 | Loss: 1.6600807905197144\n",
            "Epoch   3 | Loss: 0.9176856875419617\n",
            "Epoch   4 | Loss: 0.831023097038269\n",
            "Epoch   5 | Loss: 0.7857465147972107\n",
            "Epoch   6 | Loss: 0.7469706535339355\n",
            "Epoch   7 | Loss: 0.7105731964111328\n",
            "Epoch   8 | Loss: 0.679701566696167\n",
            "Epoch   9 | Loss: 0.6505993604660034\n",
            "Epoch  10 | Loss: 0.6153720021247864\n",
            "Epoch  11 | Loss: 0.5817742347717285\n",
            "Epoch  12 | Loss: 0.5496706962585449\n",
            "Epoch  13 | Loss: 0.5309391617774963\n",
            "Epoch  14 | Loss: 0.500799834728241\n",
            "Epoch  15 | Loss: 0.4737371504306793\n",
            "Epoch  16 | Loss: 0.45748335123062134\n",
            "Epoch  17 | Loss: 0.4339975118637085\n",
            "Epoch  18 | Loss: 0.4096483290195465\n",
            "Epoch  19 | Loss: 0.387964129447937\n",
            "Epoch  20 | Loss: 0.3663795590400696\n",
            "Epoch  21 | Loss: 0.348500519990921\n",
            "Epoch  22 | Loss: 0.3266391158103943\n",
            "Epoch  23 | Loss: 0.3139018416404724\n",
            "Epoch  24 | Loss: 0.2867174446582794\n",
            "Epoch  25 | Loss: 0.2610698938369751\n",
            "Epoch  26 | Loss: 0.2362343817949295\n",
            "Epoch  27 | Loss: 0.21415171027183533\n",
            "Epoch  28 | Loss: 0.2061079442501068\n",
            "Epoch  29 | Loss: 0.1822202503681183\n",
            "Epoch  30 | Loss: 0.1610347330570221\n",
            "Epoch  31 | Loss: 0.14815263450145721\n",
            "Epoch  32 | Loss: 0.1299654245376587\n",
            "Epoch  33 | Loss: 0.12578867375850677\n",
            "Epoch  34 | Loss: 0.11440988630056381\n",
            "Epoch  35 | Loss: 0.1180308610200882\n",
            "Epoch  36 | Loss: 0.111350417137146\n",
            "Epoch  37 | Loss: 0.09905614703893661\n",
            "Epoch  38 | Loss: 0.10490978509187698\n",
            "Epoch  39 | Loss: 0.08657162636518478\n",
            "Epoch  40 | Loss: 0.08665256202220917\n",
            "Epoch  41 | Loss: 0.06649575382471085\n",
            "Epoch  42 | Loss: 0.07306362688541412\n",
            "Epoch  43 | Loss: 0.06029703840613365\n",
            "Epoch  44 | Loss: 0.058103613555431366\n",
            "Epoch  45 | Loss: 0.06295820325613022\n",
            "Epoch  46 | Loss: 0.0699714720249176\n",
            "Epoch  47 | Loss: 0.056175652891397476\n",
            "Epoch  48 | Loss: 0.06725417077541351\n",
            "Epoch  49 | Loss: 0.05403374508023262\n",
            "Epoch  50 | Loss: 0.07560411095619202\n",
            "Epoch  51 | Loss: 0.058896712958812714\n",
            "Epoch  52 | Loss: 0.054061464965343475\n",
            "Epoch  53 | Loss: 0.04623498395085335\n",
            "Epoch  54 | Loss: 0.059826429933309555\n",
            "Epoch  55 | Loss: 0.04889858141541481\n",
            "Epoch  56 | Loss: 0.05164002254605293\n",
            "Epoch  57 | Loss: 0.048417478799819946\n",
            "Epoch  58 | Loss: 0.06241650879383087\n",
            "Epoch  59 | Loss: 0.05121435225009918\n",
            "Epoch  60 | Loss: 0.0504269152879715\n",
            "Epoch  61 | Loss: 0.0513995960354805\n",
            "Epoch  62 | Loss: 0.06424301862716675\n",
            "Epoch  63 | Loss: 0.05222346633672714\n",
            "Epoch  64 | Loss: 0.05135070160031319\n",
            "Epoch  65 | Loss: 0.04424873739480972\n",
            "Epoch  66 | Loss: 0.05591907724738121\n",
            "Epoch  67 | Loss: 0.04658261686563492\n",
            "Epoch  68 | Loss: 0.0464223250746727\n",
            "Epoch  69 | Loss: 0.04015378654003143\n",
            "Epoch  70 | Loss: 0.050070032477378845\n",
            "Epoch  71 | Loss: 0.04217249155044556\n",
            "Epoch  72 | Loss: 0.0421239510178566\n",
            "Epoch  73 | Loss: 0.036119211465120316\n",
            "Epoch  74 | Loss: 0.043972790241241455\n",
            "Epoch  75 | Loss: 0.03726847842335701\n",
            "Epoch  76 | Loss: 0.03709879145026207\n",
            "Epoch  77 | Loss: 0.031761690974235535\n",
            "Epoch  78 | Loss: 0.03148937225341797\n",
            "Epoch  79 | Loss: 0.027080923318862915\n",
            "Epoch  80 | Loss: 0.026670563966035843\n",
            "Epoch  81 | Loss: 0.023186303675174713\n",
            "Epoch  82 | Loss: 0.02739047445356846\n",
            "Epoch  83 | Loss: 0.023494236171245575\n",
            "Epoch  84 | Loss: 0.02294224314391613\n",
            "Epoch  85 | Loss: 0.01984391361474991\n",
            "Epoch  86 | Loss: 0.019305408000946045\n",
            "Epoch  87 | Loss: 0.016771724447607994\n",
            "Epoch  88 | Loss: 0.016230978071689606\n",
            "Epoch  89 | Loss: 0.014146355912089348\n",
            "Epoch  90 | Loss: 0.013615850359201431\n",
            "Epoch  91 | Loss: 0.011900368146598339\n",
            "Epoch  92 | Loss: 0.011394321918487549\n",
            "Epoch  93 | Loss: 0.009984702803194523\n",
            "Epoch  94 | Loss: 0.009513582102954388\n",
            "Epoch  95 | Loss: 0.008357519283890724\n",
            "Epoch  96 | Loss: 0.007927647791802883\n",
            "Epoch  97 | Loss: 0.0069811162538826466\n",
            "Epoch  98 | Loss: 0.00659511424601078\n",
            "Epoch  99 | Loss: 0.005821156315505505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression과는 달리 잘 수렴하는 모습을 보여주고 있습니다. 특히, 우리가 직접 gradient나 gradient descent를 구현하지 않아도 주어진 data를 잘 학습하는 코드를 PyTorch를 통해 구현할 수 있었습니다. 마지막으로 예측 결과를 살펴봅시다."
      ],
      "metadata": {
        "id": "kCMcj41Ghu7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(x))\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggGP969Bh-w",
        "outputId": "d555310d-7c24-4f40-c80b-571f44a217fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0208],\n",
            "        [1.0484],\n",
            "        [1.0156],\n",
            "        [0.0496]], grad_fn=<AddmmBackward0>)\n",
            "tensor([0, 1, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "매우 정확한 예측 결과를 보여주고 있습니다."
      ],
      "metadata": {
        "id": "GuAOyBfKiTJL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zAy7YgFDMgx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}