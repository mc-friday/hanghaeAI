{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mc-friday/hanghaeAI/blob/main/%5B3%EC%A3%BC%EC%B0%A8%5D%EC%8B%AC%ED%99%94%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbgz49PvHhLt"
   },
   "source": [
    "# [3주차] 심화과제: Pre-trained모델로 효율적인 NLP 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1LqgujQUbv6X",
    "outputId": "f8fdcd36-4c98-4861-f839-4bb44a94d5f2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.35.90)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.11.6)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.90 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.35.90)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.90->boto3) (2.8.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.90->boto3) (1.17.0)\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses datasets\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum* -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6lGiZUoPby6e"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR  # 학습률 스케줄러 추가 (Add learning rate scheduler)\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.amp import GradScaler, autocast  # 최신 AMP 모듈 (New AMP module)\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# 한글 폰트 설정\n",
    "import matplotlib.font_manager as fm\n",
    "import warnings\n",
    "fe = fm.FontEntry(\n",
    "    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf 파일이 저장되어 있는 경로\n",
    "    name='NanumGothic')                        # 이 폰트의 원하는 이름 설정\n",
    "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
    "plt.rcParams.update({'font.size': 18, 'font.family': 'NanumGothic'}) # 폰트 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cvfl_uFLIMWO"
   },
   "source": [
    "## 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_hbgPfGzi8G",
    "outputId": "4e2a6df2-2b1b-4dd5-baa5-32fc3c5d58cc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
     ]
    }
   ],
   "source": [
    "# Kaggle 데이터를 다운로드 및 경로 설정 (Download Kaggle data and set paths)\n",
    "path = kagglehub.dataset_download(\"thedevastator/unlocking-language-understanding-with-the-multin\")\n",
    "dir_path = Path(path)\n",
    "\n",
    "train_path = dir_path / \"train.csv\"  # 학습 데이터 경로 (Train data path)\n",
    "matched_val_path = dir_path / \"validation_matched.csv\"  # 검증 데이터 경로 (Validation data path)\n",
    "\n",
    "# 데이터를 pandas 데이터프레임으로 로드 (Load data into pandas DataFrame)\n",
    "train_df = pd.read_csv(train_path)\n",
    "matched_val_df = pd.read_csv(matched_val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sT6iI3LEGKBJ"
   },
   "source": [
    "## 2. 필요한 열만 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rE-y8sY9HuwP"
   },
   "outputs": [],
   "source": [
    "columns_to_keep = [\"premise\", \"hypothesis\", \"label\"]\n",
    "train_df = train_df[columns_to_keep]\n",
    "matched_val_df = matched_val_df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bF34XkoYIeEm"
   },
   "source": [
    "## 3. 결측치 확인 및 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJaUp2Vob0U-",
    "outputId": "ce59229f-e183-46c5-d66a-1484be487c81"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LOG] Missing values in datasets (before dropna):\n",
      "             premise  hypothesis  label\n",
      "train              0          40      0\n",
      "matched_val        0           0      0\n",
      "[LOG] Dataset size after dropna in train: 392662\n",
      "[LOG] Dataset size after dropna in matched_val: 9815\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-29-1afda875e2a2>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(subset=[\"premise\", \"hypothesis\", \"label\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dfs = [train_df, matched_val_df]\n",
    "data_lst = [\"train\", \"matched_val\"]\n",
    "\n",
    "# 데이터셋의 결측치 통계 출력 (Print missing value statistics)\n",
    "result = np.array([df.isna().sum().values for df in dfs])\n",
    "result = pd.DataFrame(result, index=data_lst, columns=columns_to_keep)\n",
    "print(\"[LOG] Missing values in datasets (before dropna):\")\n",
    "print(result)\n",
    "\n",
    "# 결측치가 있는 샘플 제거 (Remove rows with missing values)\n",
    "for df in dfs:\n",
    "    df.dropna(subset=[\"premise\", \"hypothesis\", \"label\"], inplace=True)\n",
    "\n",
    "# 각 데이터셋 크기 출력 (Print dataset sizes after cleaning)\n",
    "for df_name, df in zip(data_lst, dfs):\n",
    "    print(f\"[LOG] Dataset size after dropna in {df_name}: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh-tqY8WInQt"
   },
   "source": [
    "## 4. 데이터셋 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xW7ETZQzzNp2"
   },
   "outputs": [],
   "source": [
    "# PyTorch Dataset 클래스를 사용하여 MNLI 데이터셋을 정의 (Define MNLI dataset using PyTorch Dataset class)\n",
    "class MNLIDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.dataframe = dataframe  # 데이터프레임 (DataFrame)\n",
    "        self.tokenizer = tokenizer  # 토크나이저 (Tokenizer)\n",
    "        self.max_len = max_len  # 최대 길이 (Maximum length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)  # 데이터셋 길이 반환 (Return dataset length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]  # 데이터프레임에서 특정 행 가져오기 (Get a specific row)\n",
    "        premise = row[\"premise\"]\n",
    "        hypothesis = row[\"hypothesis\"]\n",
    "        label = int(row[\"label\"])  # 라벨을 정수형으로 변환 (Convert label to integer)\n",
    "\n",
    "        # 입력 문장을 토큰화 (Tokenize the input sentences)\n",
    "        inputs = self.tokenizer(\n",
    "            f\"premise: {premise} [SEP] hypothesis: {hypothesis}\",\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),  # 입력 ID 텐서 (Input ID tensor)\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),  # 어텐션 마스크 텐서 (Attention mask tensor)\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)  # 레이블 텐서 (Label tensor)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hFvSis0JLju"
   },
   "source": [
    "## 5. 토크나이저 및 DataLoader 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uyTciaPZ0KYo"
   },
   "outputs": [],
   "source": [
    "# Hugging Face DistilBERT 토크나이저 로드 (Load Hugging Face DistilBERT tokenizer)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "max_len = 128  # 최대 입력 길이 (Maximum input length)\n",
    "batch_size = 256  # 배치 크기 (Batch size)\n",
    "\n",
    "# PyTorch DataLoader 생성 (Create PyTorch DataLoaders)\n",
    "train_dataset = MNLIDataset(train_df, tokenizer, max_len)\n",
    "matched_val_dataset = MNLIDataset(matched_val_df, tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(matched_val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU7BWEbgJeKm"
   },
   "source": [
    "## 6. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XvvaAEwCznt-"
   },
   "outputs": [],
   "source": [
    "# DistilBERT 기반 MNLI 분류 모델 정의 (Define MNLI classification model based on DistilBERT)\n",
    "class MNLIClassifier(nn.Module):\n",
    "    def __init__(self, pretrain=True, freeze_encoder=True):\n",
    "        super(MNLIClassifier, self).__init__()\n",
    "        # pretrain 조건에 따라 사전 학습 가중치 사용 여부 설정\n",
    "        if pretrain:\n",
    "            self.encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")  # DistilBERT 로드 (Load DistilBERT)\n",
    "        else:\n",
    "            self.encoder = DistilBertModel(DistilBertModel.config_class())\n",
    "        self.classifier = nn.Linear(768, 3)  # 3개 클래스를 위한 분류 레이어 (Classification layer for 3 classes)\n",
    "\n",
    "        if freeze_encoder:  # encoder 동결 여부 설정\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False  # encoder 레이어 동결\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)  # DistilBERT 출력 (DistilBERT output)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]  # [CLS] 토큰 가져오기 (Extract [CLS] token)\n",
    "        return self.classifier(cls_token)  # 분류 결과 반환 (Return classification result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU1-Jt_bGsK-"
   },
   "source": [
    "## 7. 학습 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DjphVwXL00E2"
   },
   "outputs": [],
   "source": [
    "# 학습 및 검증 과정을 수행하는 함수 (Function to perform training and validation)\n",
    "def train(model, train_loader, val_loader, epochs, lr, save_path, use_amp):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)  # 옵티마이저 설정 (Set optimizer)\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.1)  # 학습률 스케줄러 설정 (Set learning rate scheduler)\n",
    "    # 클래스 가중치 추가 (클래스 불균형 보정을 위한 가중치 적용)\n",
    "    class_weights = torch.tensor([1.0, 1.5, 1.2]).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)  # 손실 함수 설정 (Set loss function)\n",
    "\n",
    "    train_loss, val_loss = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "\n",
    "    scaler = GradScaler(device='cuda') if use_amp else None  # 자동 혼합 정밀도 설정 (Set AMP)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # 모델을 학습 모드로 설정 (Set model to train mode)\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type='cuda'):  # AMP 적용 (Apply AMP)\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss.append(epoch_loss / len(train_loader))\n",
    "        train_acc.append(correct / total)\n",
    "\n",
    "        # 검증 루프 (Validation loop)\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                val_epoch_loss += loss_fn(outputs, labels).item()\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_loss.append(val_epoch_loss / len(val_loader))\n",
    "        val_acc.append(correct / total)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"[LOG] Epoch {epoch + 1}/{epochs} | Train Loss: {train_loss[-1]:.4f} | Train Acc: {train_acc[-1]:.4f} | Val Loss: {val_loss[-1]:.4f} | Val Acc: {val_acc[-1]:.4f} | Time: {epoch_time:.2f}s | Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "\n",
    "        step_lr_with_min_lr(scheduler, optimizer, min_lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "          if param_group['lr'] < min_lr:\n",
    "              param_group['lr'] = min_lr\n",
    "              print(f\"Learning rate adjusted to minimum value: {min_lr}\")\n",
    "\n",
    "    plot_metrics(train_loss, val_loss, \"Loss Curve\", \"Loss (손실)\", save_path, \"loss_curve.png\")\n",
    "    plot_metrics(train_acc, val_acc, \"Accuracy Curve\", \"Accuracy (정확도)\", save_path, \"accuracy_curve.png\")\n",
    "\n",
    "    return train_loss, val_loss, train_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfFUkEM1ZWeG"
   },
   "source": [
    "## 8. 학습 시작전 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Foks5u95ZQ1_",
    "outputId": "61238bda-744b-4fc1-d02e-c51c04971c52"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LOG] Starting training...\n",
      "Device: cuda\n",
      "Train dataset size: 392662\n",
      "Validation dataset size: 9815\n",
      "Batch size: 256\n",
      "Epochs: 10\n",
      "Learning Rate: 2e-05\n",
      "AMP Enabled: True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPU 설정 (Set GPU)\n",
    "#lr = lr=0.001  # 초기 학습률 (Initial learning rate)\n",
    "lr = 2e-5  # [Fix] Fine-tuning에 적합한 작은 학습률 사용\n",
    "min_lr = 1e-6\n",
    "epochs = 10\n",
    "save_path = \"\"\n",
    "use_amp = device.type == \"cuda\"  # AMP는 CUDA에서만 활성화 (Enable AMP only for CUDA)\n",
    "\n",
    "print(\"[LOG] Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(matched_val_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Learning Rate: {lr}\")\n",
    "print(f\"AMP Enabled: {use_amp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqQGusVIImRm"
   },
   "source": [
    "## 9. 학습 및 검증 시각화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VBEiNC6pIsG9"
   },
   "outputs": [],
   "source": [
    "# 손실 및 정확도를 시각화하는 함수 (Function to visualize loss and accuracy)\n",
    "def plot_metrics(train_values, val_values, metric_name, ylabel, save_path, file_name):\n",
    "    os.makedirs(save_path, exist_ok=True)  # 저장 경로 생성 (Create save path)\n",
    "    plt.figure(figsize=(14,10))\n",
    "    plt.plot(train_values, label=f\"Train {ylabel} (학습 {ylabel})\")\n",
    "    plt.plot(val_values, label=f\"Validation {ylabel} (검증 {ylabel})\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{metric_name} ({metric_name} 곡선)\")\n",
    "    plt.xlabel(\"Epoch (에포크)\")\n",
    "    plt.ylabel(f\"{ylabel} ({ylabel})\")\n",
    "    plt.savefig(os.path.join(save_path, file_name))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Pre-trained와 Non-pretrained 모델의 결과를 비교하여 시각화하는 함수 (Function to compare and visualize results)\n",
    "def compare_results(pre_loss, pre_acc, non_pre_loss, non_pre_acc, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)  # 저장 경로 생성 (Create save path)\n",
    "\n",
    "    # 학습 손실 비교 (Compare training loss)\n",
    "    plt.figure(figsize=(14,10))\n",
    "    plt.plot(pre_loss, label=\"Pre-trained Train Loss (사전 학습 모델 손실)\")\n",
    "    plt.plot(non_pre_loss, label=\"Non-pretrained Train Loss (비사전 학습 모델 손실)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Train Loss Comparison (학습 손실 비교)\")\n",
    "    plt.xlabel(\"Epoch (에포크)\")\n",
    "    plt.ylabel(\"Loss (손실)\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, \"train_loss_comparison.png\"))\n",
    "    plt.show()\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # 학습 정확도 비교 (Compare training accuracy)\n",
    "    plt.figure(figsize=(14,10))\n",
    "    plt.plot(pre_acc, label=\"Pre-trained Train Accuracy (사전 학습 모델 정확도)\")\n",
    "    plt.plot(non_pre_acc, label=\"Non-pretrained Train Accuracy (비사전 학습 모델 정확도)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Train Accuracy Comparison (학습 정확도 비교)\")\n",
    "    plt.xlabel(\"Epoch (에포크)\")\n",
    "    plt.ylabel(\"Accuracy (정확도)\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, \"train_accuracy_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def step_lr_with_min_lr(scheduler, optimizer, min_lr=1e-6):\n",
    "    \"\"\"\n",
    "    StepLR의 학습률을 업데이트하면서 최소 학습률을 유지하는 함수.\n",
    "    \"\"\"\n",
    "    scheduler.step()  # 기존 StepLR의 동작 수행\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr'] < min_lr:\n",
    "            param_group['lr'] = min_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GE9xUHyEIyyv"
   },
   "source": [
    "## 10. Pre-trained 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "f3gd2kDfI2kn",
    "outputId": "fa15dc56-23c7-4516-e2e6-76b75c3a6436"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LOG] Training Pre-trained model...\n",
      "[LOG] Epoch 1/10 | Train Loss: 0.6335 | Train Acc: 0.7301 | Val Loss: 0.5213 | Val Acc: 0.7886 | Time: 636.09s | Learning Rate: 0.000020\n",
      "[LOG] Epoch 2/10 | Train Loss: 0.4887 | Train Acc: 0.8050 | Val Loss: 0.4961 | Val Acc: 0.8007 | Time: 636.55s | Learning Rate: 0.000020\n",
      "[LOG] Epoch 3/10 | Train Loss: 0.4009 | Train Acc: 0.8445 | Val Loss: 0.4923 | Val Acc: 0.8120 | Time: 637.74s | Learning Rate: 0.000002\n",
      "[LOG] Epoch 4/10 | Train Loss: 0.3871 | Train Acc: 0.8503 | Val Loss: 0.4932 | Val Acc: 0.8104 | Time: 635.45s | Learning Rate: 0.000002\n",
      "[LOG] Epoch 5/10 | Train Loss: 0.3776 | Train Acc: 0.8545 | Val Loss: 0.4945 | Val Acc: 0.8126 | Time: 637.13s | Learning Rate: 0.000000\n",
      "[LOG] Epoch 6/10 | Train Loss: 0.3720 | Train Acc: 0.8571 | Val Loss: 0.4968 | Val Acc: 0.8118 | Time: 635.34s | Learning Rate: 0.000001\n",
      "[LOG] Epoch 7/10 | Train Loss: 0.3675 | Train Acc: 0.8588 | Val Loss: 0.4974 | Val Acc: 0.8131 | Time: 635.54s | Learning Rate: 0.000000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-36-6491a2ed272f>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"[LOG] Training Pre-trained model...\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mmodel_pretrained\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMNLIClassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpretrain\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m train_loss_pre, val_loss_pre, train_acc_pre, val_acc_pre = train(\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mmodel_pretrained\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muse_amp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m )\n",
      "\u001B[0;32m<ipython-input-33-5316dac687f0>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, val_loader, epochs, lr, save_path, use_amp)\u001B[0m\n\u001B[1;32m     20\u001B[0m         \u001B[0mstart_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mbatch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     23\u001B[0m             \u001B[0minput_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'input_ids'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m             \u001B[0mattention_mask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'attention_mask'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    699\u001B[0m                 \u001B[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    700\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 701\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    702\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    703\u001B[0m             if (\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 757\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    758\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    759\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory_device\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     53\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     50\u001B[0m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getitems__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m                 \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     53\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-30-6f8f2b869d3b>\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0;31m# 입력 문장을 토큰화 (Tokenize the input sentences)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m         inputs = self.tokenizer(\n\u001B[0m\u001B[1;32m     19\u001B[0m             \u001B[0;34mf\"premise: {premise} [SEP] hypothesis: {hypothesis}\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m             \u001B[0mmax_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_len\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2858\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_in_target_context_manager\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2859\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_switch_to_input_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2860\u001B[0;31m             \u001B[0mencodings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_one\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext_pair\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mall_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2861\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtext_target\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2862\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_switch_to_target_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36m_call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   2968\u001B[0m             )\n\u001B[1;32m   2969\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2970\u001B[0;31m             return self.encode_plus(\n\u001B[0m\u001B[1;32m   2971\u001B[0m                 \u001B[0mtext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2972\u001B[0m                 \u001B[0mtext_pair\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36mencode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   3044\u001B[0m         )\n\u001B[1;32m   3045\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3046\u001B[0;31m         return self._encode_plus(\n\u001B[0m\u001B[1;32m   3047\u001B[0m             \u001B[0mtext\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3048\u001B[0m             \u001B[0mtext_pair\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36m_encode_plus\u001B[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    799\u001B[0m             )\n\u001B[1;32m    800\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 801\u001B[0;31m         \u001B[0mfirst_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_input_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    802\u001B[0m         \u001B[0msecond_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_input_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_pair\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtext_pair\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    803\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36mget_input_ids\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m    766\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mget_input_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    767\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 768\u001B[0;31m                 \u001B[0mtokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    769\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_tokens_to_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    770\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001B[0m in \u001B[0;36mtokenize\u001B[0;34m(self, text, **kwargs)\u001B[0m\n\u001B[1;32m    696\u001B[0m                 \u001B[0mtokenized_text\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    697\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 698\u001B[0;31m                 \u001B[0mtokenized_text\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    699\u001B[0m         \u001B[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtokenized_text\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\u001B[0m in \u001B[0;36m_tokenize\u001B[0;34m(self, text, split_special_tokens)\u001B[0m\n\u001B[1;32m    165\u001B[0m         \u001B[0msplit_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_basic_tokenize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 167\u001B[0;31m             for token in self.basic_tokenizer.tokenize(\n\u001B[0m\u001B[1;32m    168\u001B[0m                 \u001B[0mtext\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnever_split\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall_special_tokens\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0msplit_special_tokens\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m             ):\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\u001B[0m in \u001B[0;36mtokenize\u001B[0;34m(self, text, never_split)\u001B[0m\n\u001B[1;32m    373\u001B[0m                 \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip_accents\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    374\u001B[0m                     \u001B[0mtoken\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_strip_accents\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 375\u001B[0;31m             \u001B[0msplit_tokens\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_split_on_punc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnever_split\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    376\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    377\u001B[0m         \u001B[0moutput_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mwhitespace_tokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\" \"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msplit_tokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\u001B[0m in \u001B[0;36m_run_split_on_punc\u001B[0;34m(self, text, never_split)\u001B[0m\n\u001B[1;32m    409\u001B[0m             \u001B[0mi\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 411\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    412\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    413\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_tokenize_chinese_chars\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/tokenization_distilbert.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    409\u001B[0m             \u001B[0mi\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 411\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    412\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    413\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_tokenize_chinese_chars\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(\"[LOG] Training Pre-trained model...\")\n",
    "model_pretrained = MNLIClassifier(pretrain=True, freeze_encoder=True).to(device)\n",
    "train_loss_pre, val_loss_pre, train_acc_pre, val_acc_pre = train(\n",
    "    model_pretrained, train_loader, test_loader, epochs, lr, save_path, use_amp\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGU5DpxUI4Z5"
   },
   "source": [
    "## 11. Non-pretrained 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfOfqhN0I7Ip"
   },
   "outputs": [],
   "source": [
    "print(\"[LOG] Training Non-pretrained model...\")\n",
    "model_non_pretrained = MNLIClassifier(pretrain=False, freeze_encoder=True).to(device)\n",
    "train_loss_non_pre, val_loss_non_pre, train_acc_non_pre, val_acc_non_pre = train(\n",
    "    model_non_pretrained, train_loader, test_loader, epochs, lr, save_path, use_amp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDpiHKgxI9u9"
   },
   "source": [
    "## 12. 학습 결과 비교 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nr79mleCJAUF"
   },
   "outputs": [],
   "source": [
    "# 학습 결과 비교 및 시각화 호출 (Call to compare and visualize training results)\n",
    "print(\"[LOG] Visualizing and comparing results...\")\n",
    "compare_results(\n",
    "    train_loss_pre, train_acc_pre,  # Pre-trained 모델의 결과 (Pre-trained model results)\n",
    "    train_loss_non_pre, train_acc_non_pre,  # Non-pretrained 모델의 결과 (Non-pretrained model results)\n",
    "    save_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
